# CET4的单词数据抓取

最近闲来无事，找了这个事情做了一下。因为老是没有把时间花在英语上。对那些老旧的英语学习法实在是对我们这些被高速网络腐蚀的IT人类没多大作用，加上睡眠时间少……很少有精力去背东西（何况我还有轻度的记忆衰退……但是我的选择性记忆还是很强大的，对于专业的知识几乎秒记，只要是可程序化的……）。

所以到网上找了CET4的单词表……当然只是单词表……复杂的数据对我来说也没什么用，毕竟我是要把数据存到本地的数据库中……单词3400+，好像没有CET4的要求，估计是省去了一些常用的。也没有管太多，毕竟第一笔数据原型到手了，就可以开始做。

## 数据的抓取：

选中了[Google translate](http://translate.google.com.tw/)和[Bing translate](http://www.bing.com/translator)两个站点，原因是，Google很早就开始对翻译者东西做了很多相关统计以及数据收集，可以通过这东西挖掘出很多近义词以及近义词的权重。Bing的优势是有完善的词语释义，用它来解释单词还是不错的，至少我信得过微软。

不过两个站点都让我感觉到头疼，不说他们提供的API……，这东西麻烦是了，还要有认证的key，Google更加坑，还要钱的说……所以抓包才是王道，数据简单明了，方便测试查找错误。

难点在于Bing translate的技术是微软的Asp.NET技术做的，所以AJAX下来的数据都是HTML数据，让我感觉到蛋蛋的哀伤，并不是HTML数据不好解析，关键是作为一个词典，它的模式有很多种，有时候它输出的数据你压根就不知道怎么搞怎么分解……不过也还好，只要跟着Bing输出到页面数据的规则进行分块就好，即时到后面发现数据模型有误，也能快速的修复。

Google的数据包就简单多了，javascript直接eval就过，但是GET的很多字段模模糊糊的压根就不懂是什么意识，索性影响不大，只要抓数据包的时候别带上Cookie就好。但是Google有一个坑，来至华夏……你懂的，所以要用两个base url，一个是[https://translate.google.cn/translate_a/t](#)，一个是[http://translate.google.com.tw/translate_a/t](#)，后者比较稳定，抓它2000+的数据包不会出岔子，不过一旦出了岔子，就要用前面的那个线路才行，而前面那个抓它几百个就断了……搞得我很愤世，一开始的时候不了解问题一直在两个线路间切换，以为是GET的字段有问题……

## 数据库：

目前使用的是 **mongodb**，本来是想用 **SQLite**（想做到客户端化）的，但是 **mongodb**支持json的数据，对于前期的开发来说跟方便，因为我是用nodejs来搞的。当然为了后期方便，我还是把三千多条数据做了备份，到数据处理完整了，确定结构了，再导入 **SQLite**中。

之前对数据库接触一直不多，用的是 **SQL Server**，SQL语句也只是学了基本的。但真心用的不多，做的东西也没那么多性能要求，所以就没有去深究这些细节。但数据库毕竟只是工具，用纯粹的文件和文件夹配合也可以做到可视化存储……（不考虑性能的话），所以了解一个数据库的强项是第一步， **mongodb**搞的是 **NoSQL**，这个就不用我多说了， **SQLite**大家都知道，小巧精悍。不过说实话 **mongodb**官方的帮助文档写得实在难看……没有系统地去学习，目前还没必要在这东西上花太多时间，搞不好到时候就不是在搞CET4的单词而是玩数据库去了。-_-!!……在网上找了基本的增删查的方法，前期开发够用了，直接开始封装这些方法。

javascript的特性就是各种回调，我需要实现的是for循环去抓取3000+的数据……而数据库的写入读取是异步的，以我目前的技术没发搞得那么全自动，不知道怎么判定最后一条数据处理完成然后再处理出错的，所以一直是手动将抓包日志处理，理出出错的单词，重新抓取过一次……当然我也可以直接在出错的时候直接进行处理重新抓取……不过怕对错误的类型没有充分了解，到时候死循环就不好了……

**mongodb**数据库的开开关关一直让我很头疼，到现在还是不大理解，虽然说数据库开了一次就不用再关闭了……但是有时候开着没关的话，写入就卡着了？关掉的话有时候又开不起来……当然这99%是我技术问题……尼玛这让我情何以堪。

不过还好到最后数据还是蛮ok的抓下来，然后就是把冗余项清理干净。

```js
	//+清空name相同的项
	//collection.find是我封装的方法，和原生的类似，只是又多了一层异步来提高性能
	collection.find({collection:"CET4.1"},function(err,data){
		console.log("befor ensureIndex");
		console.log(data.length);
		operateMongo("CET4.1",function(err,col){
			col.ensureIndex({name: 1, nodes: 1}, {unique: true, dropDups: true},function(){
					collection.find({collection:"CET4.1"},function(err,data){
						console.log("after ensureIndex");
						console.log(data.length);
						collection.close();
					})
				});
			});
	});
	// -清空name相同的项
```

---

另外就是要谈到一些性能问题……3000+的循环对js来说不算什么，但是最可怕的是循环加异步回调……如果用闭包来处理的话，内存会爆掉……nodejs直接吃掉我1.2G的内存我会乱说？当我洗个脸回来看到任务管理器里面的nodejs的时候，我一直在掂量到底是120M还是120M还是120M…………(-_-)。所以[JSHint](http://www.jshint.com/)才会在规范中强烈不建议用户在循环中定义函数（很容易形成闭包），要的话就要在循环外部定义一个函数来用，当然，你也可以开启nodejs的ES6的部分功能，使用```let```关键字可以直接避免闭包的发生。

还有磁盘IO的问题。数据库的写入，我在异步的基础上又封装了一下，在3000+的数据写入中，实际上就写入了不到100次（本来想find也做异步的，但是想想没什么必要），我是定义了300mm后如果没有等到下一条写入命令的话才去写入，我的网速不慢，300mm够用了，我也不想一下子缓存3000+的数据，内存吃紧，反正是异步的IO。不过为了 **SQLite**的应用，将数据缓存到磁盘上，这我真的没有什么好的办法去解决其性能。文件流都是一个个来的……3000+的文件，3000+的开文件流关文件流，并不像数据库一两个文件在整合所有数据。IO曲线飘得我老心疼的……我实在没法想到什么好的方法。获取真的得吧数据在 **mongodb**中搞定了，再输出到文件夹中……但是当时直接把3000+的文件按时间排列去看数据处理。看哪些数据写入失败，单独拿出来测试……现在想起来各种坑。

现在是打算用这些数据做了个WebGL的应用，将数据关联起来：汉字、图片、视频（这个有点虚，待定），还有句子（这个可以有，找几本名书，分解一下就ok）。
不过这得分布来，现在是把数据导出成Sublime Text的提示插件，输入拼音输出汉字提示和单词（Tab后）、输入单词输出简单中文释义以及详细信息（Tab后）。就不打算搞到网上去了，直接一个压缩包放着就好……找一天都传到Github上去。

**2013/5/12 16:12:57**